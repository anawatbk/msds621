{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parrt/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.datasets import load_boston, load_iris, load_wine, load_digits, \\\n",
    "                             load_breast_cancer, load_diabetes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from rfpimp import *\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "if LooseVersion(sklearn.__version__) >= LooseVersion(\"0.24\"):\n",
    "    # In sklearn version 0.24, forest module changed to be private.\n",
    "    from sklearn.ensemble._forest import _generate_unsampled_indices\n",
    "    from sklearn.ensemble import _forest as forest\n",
    "else:\n",
    "    # Before sklearn version 0.24, forest was public, supporting this.\n",
    "    from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "    from sklearn.ensemble import forest\n",
    "\n",
    "from sklearn import tree\n",
    "from dtreeviz.trees import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rent(n=None, bootstrap=False):\n",
    "    df_rent = pd.read_csv(\"data/rent-ideal.csv\")\n",
    "    if n is None:\n",
    "        n = len(df_rent)\n",
    "    df_rent = df_rent.sample(n, replace=bootstrap)\n",
    "    X = df_rent[['bedrooms','bathrooms','latitude','longitude']]\n",
    "    y = df_rent['price']\n",
    "    return X, y\n",
    "\n",
    "def boston():\n",
    "    boston = load_boston()\n",
    "    X = boston.data\n",
    "    y = boston.target\n",
    "    features = boston.feature_names\n",
    "    df = pd.DataFrame(data=X,columns=features)\n",
    "    df['y'] = y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "Get the `rent-ideal.csv`  data file from canvas and store in the data directory underneath your notebook  directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = rent()\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train random forests of different sizes\n",
    "\n",
    "As we increase the number of trees in the forest, we initially see model bias going down. It will asymptotically approach some minimum error on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to train a random forest  that has a single tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Compute the MAE for the training and the testing set, printing them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train = mean_absolute_error(...)\n",
    "mae = mean_absolute_error(...)\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Run the training and testing cycle several times to see the variance: the test scores bounce around a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Increase the number of trees (`n_estimators`) to 2, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=2)\n",
    "rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice the both test MAE scores going down and bouncing around less from run to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Why does the MAE score go down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    With 2 trees, the chances are that the random forest will have seen (trained on) more of the original training set, despite bootstrapping.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Increase the number of trees (`n_estimators`) to 10, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=10)\n",
    "rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  What you notice about the MAE scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "They are getting smaller.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  After running several times, what else do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    With 10 trees, the prediction from run to run varies a lot less. We have reduced variance,  improving generality.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Increase the number of trees (`n_estimators`) to 200, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=200)\n",
    "%time rf.fit(X_train, y_train) # how long does this take?\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  What you notice about the MAE scores from a single run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "They are a bit smaller, but not by much.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Notice that it took a long time to train, about 10 seconds.  Do the exact same thing again but this time use `n_jobs=-1` as an argument to the `RandomForestRegressor` constructor.\n",
    "\n",
    "This tells the library to use all processing cores available on the computer processor. As long as the data is not too huge (because it must pass it around), it often goes much faster using this argument. It should take less than two seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  What you notice about the MAE scores from SEVERAL runs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The variance is even lower (tighter).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining model size and complexity\n",
    "\n",
    "The structure of a tree is affected by a number of hyper parameters, not just the data. Goal in the section is to see the effect of altering the number of samples per leaf and the maximum number of candidate features per split. Let's start out with a handy function that uses some  support code from rfpimp to examine tree size and depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showsize(ntrees, max_features=1.0, min_samples_leaf=1):\n",
    "    rf = RandomForestRegressor(n_estimators=ntrees,\n",
    "                               max_features=max_features,\n",
    "                               min_samples_leaf=min_samples_leaf,\n",
    "                               n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    n = rfnnodes(rf)                # from rfpimp\n",
    "    h = np.median(rfmaxdepths(rf))  # rfmaxdepths from rfpimp\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:6.1f}$, test {mae:6.1f}$ using {n:9,d} tree nodes with {h:2.0f} median tree height\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single tree, we see about 21,000 nodes and a tree height of around 35:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showsize(ntrees=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Look at the metrics for 2 trees and then 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "showsize(ntrees=2)\n",
    "showsize(ntrees=100)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why does the median height of a tree stay the same when we increase the number of trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "While the number of nodes increases with the number of trees, the height of any individual tree will stay the same because we have not fundamentally changed how it is constructing a single tree.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of increasing min samples / leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Loop around a call to `showsize()` with 10 trees and min_samples_leaf=1..10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(...):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "for i in range(1,10+1):\n",
    "    showsize(ntrees=10, min_samples_leaf=i)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why do the median height of a tree and number of total nodes decrease as we increase the number of samples per leaf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Because  when the sample size gets down to `min_samples_leaf`, splitting stops, which prevents the tree from getting taller. It also restricts how many nodes total get created for the tree.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Why does the MAE error increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "If we include more observations in a single leaf, then the average is taken over more samples. That average is a more general prediction but less accurate.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty clear from that print out that `min_samples_leaf=1` is the best choice because it gives the minimum validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of reducing max_features (rent data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Do another loop from `max_features` = 4 down to 1, with 1 sample per leaf. (There are 4 total features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X_train.shape[1]\n",
    "for i in range(...):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=10, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "p = X_train.shape[1]\n",
    "for i in range(p,0,-1):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=10, max_features=i)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data set,  changing the available candidate features that each split does not seem to be important as the validation error does not change, nor does the height of the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine effects of hyper parameters on Boston data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston = boston()\n",
    "df_boston.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_boston.drop('y', axis=1), df_boston['y']\n",
    "y *= 1000 # y is \"Median value of owner-occupied homes in $1000's\" so multiply by 1000\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the metric `showsize()` function to see how many trees we should use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,5,30,50,100,150,300]:\n",
    "    print(f\"{i:3d} trees: \", end='')\n",
    "    showsize(ntrees=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the sweet spot on the validation error is probably 30 trees as it gets a low validation error and has a fairly small set of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the effect of increasing the minimum samples per leaf from 1 to 10 as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10+1):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=30, min_samples_leaf=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error goes up dramatically but the validation error doesn't get too much worse.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Which min samples per leaf would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "After running a few times, it seems that using `min_samples_leaf`=3 or 4  is best for the validation error.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a loop from the maximum number of features down to 1 for `max_features` to see the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X_train.shape[1]\n",
    "for i in range(p,0,-1):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=30, max_features=i, min_samples_leaf=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Which max features would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "After running a few times, it seems that using `max_features`=6 or 5 gets best validation error.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the final model would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showsize(ntrees=30, max_features=6, min_samples_leaf=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF prediction confidence\n",
    "\n",
    "A random forest is a collection of decision trees, each of which contributes a prediction. The forest averages those predictions to provide the overall prediction (or takes most common vote for classification). Let's dig inside the random forest to get the individual trees out and ask them what their predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Train a random forest with 10 trees on `X_train`, `y_train`.  Use `for t in rf.estimators_` to iterate through the trees making predictions with `t` not `rf`. Print out the usual MAE scores for each tree predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for t in ...:\n",
    "    mae_train = ...\n",
    "    mae = ...\n",
    "    print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for t in rf.estimators_:\n",
    "    mae_train = mean_absolute_error(y_train, t.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, t.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it bounces around quite a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Select one of the `X_test` rows and print out the addicted rent price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ... # pick single test case\n",
    "x = x.values.reshape(1,-1) # Needs to be a one-row matrix\n",
    "\n",
    "print(f\"{x} => {rf.predict(x)}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "x = X_test.iloc[3,:] # pick single test case\n",
    "x = x.values.reshape(1,-1)\n",
    "print(f\"{x} => {rf.predict(x)}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Now let's see how the forest came to that conclusion. Compute the average of the predictions obtained from every tree.  \n",
    "\n",
    "Compare that to the prediction obtained directly from the random forest (`rf.predict(X_test)`). They should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ...\n",
    "print(f\"{x} => {y_pred}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "y_pred = np.mean([t.predict(x) for t in rf.estimators_])\n",
    "print(f\"{x} => {y_pred}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Compute the standard deviation of the tree estimates and print that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "np.std([t.predict(x) for t in rf.estimators_])\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the standard deviation, the more tightly grouped the predictions were, which means we should have more confidence in our answer. \n",
    "\n",
    "Different records will often have different standard deviations, which means we could have different levels of confidence in the various answers. This might be helpful to a bank for example that wanted to not only predict whether to give loans, but how confident the model was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering bootstrap size\n",
    "\n",
    "Jeremy Howard has an awesome trick that he uses during development of a model to convince sklearn random forests to use less than $n$ observations from the training data to train each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jeremy_trick_RF_sample_size(n):\n",
    "    if LooseVersion(sklearn.__version__) >= LooseVersion(\"0.24\"):\n",
    "        forest._generate_sample_indices = \\\n",
    "            (lambda rs, n_samples, _:\n",
    "             forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "    else:\n",
    "        forest._generate_sample_indices = \\\n",
    "            (lambda rs, n_samples: forest.check_random_state(rs).randint(0, n_samples, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: There are about 38,000 training records, change that to 19,000 and check the accuracy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeremy_trick_RF_sample_size(round(len(X_train)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit less accurate, but it's faster. (Down to 1.27s from 1.86s on my machine.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Why is it less accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Each tree is seeing less of the data set during training.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Turn off bootstrapping by adding `bootstrap=False` to the constructor of the model. This means that it will subsample rather than bootstrap. Remember that bootstrapping gets about two thirds of the data because of replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1, bootstrap=False)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That brings the accuracy backup a little bit for the test set but very much so for the training MAE score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Drop that size to one third of the training records then retrain and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeremy_trick_RF_sample_size(round(len(X_train)/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine is twice as fast as the full bootstrap but continues to have very tight variance because of the number of trees. The accuracy is lower, however, about what we get for the  usual random forest with two trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Before we forget, let's reset the bootstrapping size back to the usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jeremy_trick_reset_RF_sample_size():\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeremy_trick_reset_RF_sample_size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
