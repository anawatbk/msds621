{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.datasets import load_boston, load_iris, load_wine, load_digits, \\\n",
    "                             load_breast_cancer, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn import tree\n",
    "from dtreeviz.trees import *\n",
    "from dtreeviz.models.shadow_decision_tree import ShadowDecTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mse_leaves(X,y,max_depth):\n",
    "    t = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    t.fit(X,y)\n",
    "    shadow = ShadowDecTree.get_shadow_tree(t, X, y, feature_names=['sqfeet'], target_name='rent')\n",
    "    root, leaves, internal = shadow._get_tree_nodes()\n",
    "    # node2samples = shadow._get_tree_nodes()_samples()\n",
    "    # isleaf = shadow.get_node_type(t)\n",
    "    n_node_samples = t.tree_.n_node_samples\n",
    "\n",
    "    mse = 99.9#mean_squared_error(y, [np.mean(y)]*len(y))\n",
    "    print(f\"Root {0:3d} has {n_node_samples[0]:3d} samples with MSE ={mse:6.2f}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    avg_mse_per_record = 0.0\n",
    "    node2samples = shadow.get_node_samples()\n",
    "    for node in leaves:\n",
    "        leafy = y[node2samples[node.id]]\n",
    "        n = len(leafy)\n",
    "        mse = mean_squared_error(leafy, [np.mean(leafy)]*n)\n",
    "        avg_mse_per_record += mse * n\n",
    "        print(f\"Node {node.id:3d} has {n_node_samples[node.id]:3d} samples with MSE ={mse:6.2f}\")\n",
    "\n",
    "    avg_mse_per_record /= len(y)\n",
    "    print(f\"Average MSE per record is {avg_mse_per_record:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure to get latest dtreeviz**\n",
    "\n",
    "`pip install -U dtreeviz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_cars = pd.read_csv(\"data/cars.csv\")\n",
    "X, y = df_cars[['ENG']], df_cars['MPG']\n",
    "df_cars.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(max_depth=1)\n",
    "dt.fit(X, y)\n",
    "\n",
    "rtreeviz_univar(dt, X, y,\n",
    "                feature_names='Horsepower',\n",
    "                markersize=5,\n",
    "                mean_linewidth=1,\n",
    "                target_name='MPG',\n",
    "                fontsize=9,\n",
    "                show={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What is the MSE between y and predicted $\\hat{y} = \\overline{y}$?\n",
    "\n",
    "Hints: You can use function `mean_squared_error(` $y$,$\\hat{y}$ `)`; create a vector of length $|y|$ with $\\overline{y}$ as elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "mean_squared_error(y, [np.mean(y)]*len(y)) # about 60.76\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Where would you split this if you could only split once?  Set the `split` variable to a reasonable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The split location that gets most pure subregion might be about split = 200 HP because the region to the right has a relatively flat MPG average.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alter the rtreeviz_univar() call to show the split with arg show={'splits'}**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Solution</summary>\n",
    "<pre>\n",
    "rtreeviz_univar(dt, X, y,\n",
    "                feature_names='Horsepower',\n",
    "                markersize=5,\n",
    "                mean_linewidth=1,\n",
    "                target_name='MPG',\n",
    "                fontsize=9,\n",
    "                show={'splits'})\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What are the MSE values for the left, right partitions?\n",
    "\n",
    "Hints: Get the y values whose `X['ENG']` are less than `split` into `lefty` and those greater than or equal to `split` into `righty`.  The split introduces two new children that are leaves until we (possibly) split them; the leaves predict the mean of their samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefty = ...;   mleft = ...\n",
    "righty = ...; mright = ...\n",
    "\n",
    "mse_left = ...\n",
    "mse_right = ...\n",
    "\n",
    "mse_left, mse_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    Should be (35.68916307096633, 12.770261374699789)<p>\n",
    "<pre>\n",
    "lefty = y[X['ENG']<split]\n",
    "righty = y[X['ENG']>=split]\n",
    "mleft = np.mean(lefty)\n",
    "mright = np.mean(righty)\n",
    "\n",
    "mse_left = mean_squared_error(lefty, [mleft]\\*len(lefty))\n",
    "mse_right = mean_squared_error(righty, [mright]\\*len(righty))\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Compare the MSE values for overall y and the average of the left, right partition MSEs (which is about 24.2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "After the split the MSE of the children is much lower than before the split, therefore, it is a worthwhile split.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Set the split value to 100 and recompare MSE values for y, left, and right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "With split=100, mse_left, mse_right become 33.6 and 41.0. These are still less than the y MSE of 60.7 so worthwhile but not nearly as splitting at 200.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of deeper trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the sequence of tree depths 1..6 for horsepower vs MPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = df_cars[['ENG']].values\n",
    "y = df_cars['MPG'].values\n",
    "\n",
    "fig, axes = plt.subplots(1,6, figsize=(14,3), sharey=True)\n",
    "for i,ax in enumerate(axes.flatten()):\n",
    "    dt = DecisionTreeRegressor(max_depth=i+1)\n",
    "    dt.fit(X, y)\n",
    "    t = rtreeviz_univar(dt,\n",
    "                        X, y,\n",
    "                        feature_names='Horsepower',\n",
    "                        markersize=5,\n",
    "                        mean_linewidth=1,\n",
    "                        target_name='MPG' if i==0 else None,\n",
    "                        fontsize=9,\n",
    "                        show={'splits'},\n",
    "                        ax=ax)\n",
    "    ax.set_title(f\"Depth {i+1}\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Focusing on the orange horizontal lines, what do you notice as more splits appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "With depth 1, model is biased due to coarseness of the approximations (just 2 leaf means).  Depth 2 gets much better approximation, so bias is lower. As we add more depth to tree, number of splits increases and these appear to be chasing details of the data, decreasing bias on training set but also hurting generality.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Consider the MSE for the 4 leaves of a depth 2 tree and 15 leaves of a depth 4 tree.  What happens to the average MSE per leaf?  What happens to the leaf sizes and how is it related to average MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mse_leaves(df_cars[['ENG']], df_cars['MPG'], max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mse_leaves(df_cars[['ENG']], df_cars['MPG'], max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The average MSE is much lower as we increase depth because that allows the tree to isolate pure/more-similar regions.  This also shrinks leaf size since we are splitting more as the tree deepens.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the plot of the CYL feature (num cylinders) vs MPG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = df_cars[['CYL']].values\n",
    "y = df_cars['MPG'].values\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(7,2.5), sharey=True)\n",
    "depths = [1,2,10]\n",
    "for i,ax in enumerate(axes.flatten()):\n",
    "    dt = DecisionTreeRegressor(max_depth=depths[i])\n",
    "    dt.fit(X, y)\n",
    "    t = rtreeviz_univar(dt,\n",
    "                        X, y,\n",
    "                        feature_names='Horsepower',\n",
    "                        markersize=5,\n",
    "                        mean_linewidth=1,\n",
    "                        target_name='MPG' if i==0 else None,\n",
    "                        fontsize=9,\n",
    "                        show={'splits','title'},\n",
    "                        ax=ax)\n",
    "    ax.set_title(f\"Depth {i+1}\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Explain why the graph looks like a bunch of vertical bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The x values are integers and will clump together. Since there are many MPG values at each int, you get vertical clumps of data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why don't we get many more splits for depth 10 vs depth 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Once each unique x value has a \"bin\", there are no more splits to do.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why are the orange predictions bars at the levels they are in the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Decision tree leaves predict the average y for all samples in a leaf.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "df_wine = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "df_wine.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "feature_names = list(wine.feature_names)\n",
    "class_names = list(wine.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_wine[['flavanoids']].values\n",
    "y = wine.target\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=1)\n",
    "dt.fit(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,1.8))\n",
    "ct = ctreeviz_univar(dt, X, y,\n",
    "                     feature_names = 'flavanoids',\n",
    "                     class_names=class_names,\n",
    "                     target_name='Wine',\n",
    "                     nbins=40, gtype='strip',\n",
    "                     fontsize=9,\n",
    "                     show={},\n",
    "                     colors={'scatter_marker_alpha':1, 'scatter_marker_alpha':1},\n",
    "                     ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Where would you split this (vertically) if you could only split once?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The split location that gets most pure subregion might be about 1.5 because it nicely carves off the left green samples.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alter the code to show the split with arg show={'splits'}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "X = df_wine[['flavanoids']].values\n",
    "y = wine.target\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=1)\n",
    "dt.fit(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,1.8))\n",
    "ct = ctreeviz_univar(dt, X, y,\n",
    "                     feature_names = 'flavanoids',\n",
    "                     class_names=class_names,\n",
    "                     target_name='Wine',\n",
    "                     nbins=40, gtype='strip',\n",
    "                     fontsize=9,\n",
    "                     show={'splits'},\n",
    "                     colors={'scatter_marker_alpha':1, 'scatter_marker_alpha':1},\n",
    "                     ax=ax)\n",
    "plt.show()\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** For max_depth=2, how many splits will we get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "3. We get one split for root and then with depth=2, we have 2 children that each get a split.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Where would you split this graph in that many places?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Once we carve off the leftmost green, we would want to isolate the blue in between 1.3 and 2.3.  The other place to split is not obvious as there is no great choice. (sklearn will add a split point at 1.0)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alter the code to show max_depth=2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "X = df_wine[['flavanoids']].values\n",
    "y = wine.target\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2)\n",
    "dt.fit(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,1.8))\n",
    "ct = ctreeviz_univar(dt, X, y,\n",
    "                     feature_names = 'flavanoids',\n",
    "                     class_names=class_names,\n",
    "                     target_name='Wine',\n",
    "                     nbins=40, gtype='strip',\n",
    "                     fontsize=9,\n",
    "                     show={'splits'},\n",
    "                     colors={'scatter_marker_alpha':1, 'scatter_marker_alpha':1},\n",
    "                     ax=ax)\n",
    "plt.show()\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the gini impurity for left and right sides for a depth=1 tree that splits flavanoids at 1.3. Here's a function that computes the value:\n",
    "\n",
    "$$\n",
    "I_G = \\sum_{i=1}^k \\sum_{j \\neq i}^k P_j = 1 - \\sum_{i=1}^{k} P_i^2\n",
    "$$\n",
    "\n",
    "where $k$ is the number of classes and $P_i$ is the probability of seeing class $i$ in our target $y$ (it's the ratio of $\\frac{|y[y==i]|}{|y|}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(x):\n",
    "    \"See https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\"\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    n = len(x)\n",
    "    return 1 - np.sum( (counts / n)**2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Using that function, what is the gini impurity for the overall y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "gini(y) # about 0.66\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all y values for rows where `df_wine['flavanoids']`<1.3 into variable `lefty` and `>=` into `righty`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefty = ...\n",
    "righty = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "lefty = y[df_wine['flavanoids']&lt;1.3]\n",
    "righty = y[df_wine['flavanoids']&gt;=1.3]\n",
    "</pre>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What are the gini values for left and right partitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "gini(lefty), gini(righty) # about 0.27, 0.53\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What can we conclude about the purity of left and right? Also, compare to gini  for all y values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Left partition is much more pure than right but right is still more pure than original gini(y). We can conclude that the split is worthwhile as the partition would let us give more accurate predictions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_wine[['alcohol','flavanoids']].values\n",
    "y = wine.target\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=1)\n",
    "dt.fit(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,3))\n",
    "ct = ctreeviz_bivar(dt, X, y,\n",
    "                     feature_names = ['alcohol','flavanoid'], class_names=class_names,\n",
    "                     target_name='iris',\n",
    "                     show={},\n",
    "                     colors={'scatter_marker_alpha':1, 'scatter_marker_alpha':1},\n",
    "                     ax=ax\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Which variable and split point would you choose if you could only split once?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Because the blue dots are spread vertically, a horizontal split won't be very good. Hence, we should choose variable proline.  The best split will carve off the blue dots, leaving the yellow and green mixed up.  A split at proline=12.7 seems pretty good.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the code to view the splits and compare your answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Which variable and split points would you choose next for depth=2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Once we carve off most of the blue vertically, we should separate the yellow by choosing flavanoid=1.7 to split horizontally. NOTICE, however, that the 2nd split will not be across entire graph since we are splitting the region on the right.  Splitting on the left can be at flavanoid=1 so we isolate the green from blue on left.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the code to view the splits for depth=2 and compare your answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini\n",
    "\n",
    "Let's examine gini impurity for a different pair of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_wine[['proline','flavanoids']].values\n",
    "y = wine.target\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=1)\n",
    "dt.fit(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,3))\n",
    "ctreeviz_bivar(dt, X, y,\n",
    "               feature_names = ['proline','flavanoid'],\n",
    "               class_names=class_names,\n",
    "               target_name='iris',\n",
    "               show={'splits'},\n",
    "               colors={'scatter_marker_alpha':1, 'scatter_marker_alpha':1},\n",
    "               ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all y values for rows where the split var is less than the split value into variable `lefty` and those `>=` into `righty`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefty = ...\n",
    "righty = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "lefty = y[df_wine['proline']&lt;750]\n",
    "righty = y[df_wine['proline']&gt;=750]\n",
    "</pre>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the gini for y, lefty, righty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "gini(y), gini(lefty), gini(righty)\n",
    "</pre>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a single tree and print out the training accuracy (num correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DecisionTreeClassifier()\n",
    "t.fit(df_wine, y)\n",
    "accuracy_score(y, t.predict(df_wine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfpimp import *\n",
    "I = importances(t, df_wine, y)\n",
    "plot_importances(I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
