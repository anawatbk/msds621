{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring regularization for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goal of this lab is to explore the effect of regularization on the coefficients and accuracy of linear regression models for a toy (Ames housing) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_wine, load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, drop columns with missing values, one-hot encode categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ames = pd.read_csv(\"ames.csv\") # in same directory as this notebook\n",
    "df_ames.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore columns with missing values for this exercise\n",
    "cols_with_missing = df_ames.columns[df_ames.isnull().any()]\n",
    "cols = set(df_ames.columns) - set(cols_with_missing)\n",
    "\n",
    "X = df_ames[cols]\n",
    "X = X.drop('LotArea', axis=1) # has some outliers, let's drop for stability in this lab\n",
    "X = X.drop('SalePrice', axis=1)\n",
    "y = df_ames['SalePrice']\n",
    "\n",
    "X = pd.get_dummies(X) # dummy encode categorical variables\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine coefficients of nonregularized OLS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Split off validation/test set and train unregularized linear regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Compare the R^2 on training and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{lm.score(X_train, y_train):.2f} R^2 on training set\")\n",
    "print(f\"{lm.score(X_test, y_test):.2f} R^2 on test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why is the testing score much worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Because the unconstrained coefficients of the OLS model do not generalize well; they are over fit to the training set, which makes them performed poorly on the unseen test set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Predict $\\overline{y}$ computed from training set instead of using the model; what is the R^2? on test set?**\n",
    "\n",
    "R^2 should give 0 as the neutral value when the prediction is no better and no worse than simply predicting the average of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.full(shape=(len(y_test),1), fill_value=np.mean(y_train)) # get vector of y_train bar\n",
    "r2_score(y_pred, y_test) # how well do we predict y_test using y_train bar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Extract $\\beta_1, ..., \\beta_p$ and count how many are close to 0**\n",
    "\n",
    "Note: `sum(np.abs(x) < 1e-5)` is a decent way to check for values of `x` close to zero but not necessarily zero.  There is also `numpy.isclose()` but that is too strict (requires numbers to be really close to zero) for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_beta = lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.abs(lm_beta) < 1e-5) # how many close to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Plot the coefficient index versus the value**\n",
    "\n",
    "R^2 should give 0 as the neutral value when the prediction is no better and no worse than simply predicting the average of the training set.  The coefficients should look something like the following images, depending on which training samples are chosen:\n",
    "\n",
    "<img src=\"ames-ols.png\" width=\"300\"><img src=\"ames-ols-2.png\" width=\"300\">\n",
    "\n",
    "(There is also some randomness probably due to how the sklearn regression model deletes collinear columns to prevent the symbolic solution that finds coefficients from failing.)\n",
    "\n",
    "The following function is a handy way to plot the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotbeta(beta, which, yrange=(-20_000, 20_000),fontsize=10, xlabel=True, ylabel=True, ax=None):\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(1,1,figsize=(4,2.5))\n",
    "    ax.bar(range(len(beta)),beta)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(\"Coefficient $\\\\beta_i$ for $i \\\\geq 1$\", fontsize=fontsize)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(\"Coefficient value\", fontsize=fontsize)\n",
    "    if yrange is not None:\n",
    "        ax.set_ylim(*yrange)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "    plt.gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:.0f}'))\n",
    "    ax.set_title(f\"{which} $\\\\overline{{\\\\beta}}$={np.mean(beta):.0f}, $\\\\sigma(\\\\beta)$={np.std(beta):.1f}\", fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbeta(lm_beta, \"OLS\", yrange=None)\n",
    "#plt.tight_layout(); plt.savefig(\"ames-ols-2.png\",dpi=150,bbox_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Both L1 and L2 regularization benefit from normalizing variables; sklearn has an option to do this as part of the training process, but let's do it explicitly as a way to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "def normalize(X): \n",
    "    for colname in X.columns:\n",
    "        if is_numeric_dtype(X[colname]):\n",
    "            u = np.mean(X[colname])\n",
    "            s = np.std(X[colname])\n",
    "            X[colname] = (X[colname] - u) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = df_ames.columns[df_ames.isnull().any()]\n",
    "cols = set(df_ames.columns) - set(cols_with_missing)\n",
    "\n",
    "X = df_ames[cols].drop('SalePrice', axis=1)\n",
    "y = df_ames['SalePrice']\n",
    "\n",
    "normalize(X) # do this before getting dummy variables\n",
    "X = pd.get_dummies(X) # make sure to normalize before you get the dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(3)  # These values should be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the sklearn model constructors call the parameter `alpha` not `lmbda`.\n",
    "lmbda = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 (Lasso) regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Train an L1 (lasso) linear regression model using lmbda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = ...\n",
    "# fit model to X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "lasso = Lasso(alpha=lmbda, tol=.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Examine the training and testing scores**\n",
    "\n",
    "Note that the sklearn model constructors call the parameter `alpha` not `lmbda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{lasso.score(X_train, y_train):.2f} R^2 on training set\")\n",
    "print(f\"{lasso.score(X_test, y_test):.2f} R^2 on test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why is the testing score much worse than the training score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Because the lambda hyperparameter is small, 5, the coefficients are similar to those of the OLS model which do not generalize well; the coefficients are overfit to the training set, which makes them performed poorly on the unseen test set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why is the training score about the same for OLS and regularized regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Regularization is mostly about generalization and training scores tell us nothing about generalization.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.  Count how many $\\beta_{1..p}$ coefficients are (close to) zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(...) # how many close to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "sum(np.abs(lasso.coef_) < 1e-5) # how many close to 0?\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Plot the  $\\beta_{1..p}$ coefficients using `plotbeta`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbeta(lasso.coef_, \"Lasso $\\lambda=\"+str(lmbda)+\"$: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 (Ridge) regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Train an L2 (Ridge) linear regression model using lmbda**\n",
    "\n",
    "Note that the sklearn model constructors call the parameter `alpha` not `lmbda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = ...\n",
    "# fit to X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "ridge = Ridge(alpha=lmbda)\n",
    "ridge.fit(X_train, y_train)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Examine the training and testing scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{ridge.score(X_train, y_train):.2f} R^2 on training set\")\n",
    "print(f\"{ridge.score(X_test, y_test):.2f} R^2 on test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Why are the R^2 scores for L2 similar to L1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "L1 and L2 Regularization is solving the same problem: constraining the size of the coefficients. The only difference is the metric used. There both effective and so we would expect there scores to be similar.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.  Count how many $\\beta_{1..p}$ coefficients are (close to) zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.abs(ridge.coef_) < 1e-5) # how many close to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Plot the  $\\beta_{1..p}$ coefficients using `plotbeta`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbeta(ridge.coef_, \"Ridge $\\lambda=\"+str(lmbda)+\"$: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Compare the standard deviation of the coefficients for L1 and L2. What do you notice visually?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The L2 coefficients are smaller and have smaller standard deviation; they are more tightly grouped. Also note that there are fewer L2 coefficients at 0.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of $\\lambda$ on regularization and R^2 scores\n",
    "\n",
    "The goal of the following code snippets is to help you visualize how the $\\lambda$ regularization parameter affects model parameters and associated training and testing scores. There are a number of important questions to answer following the code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,7,figsize=(13.5,1.8), sharey=True)\n",
    "for i,lmbda in enumerate([1,10,100,200,500,1000,2000]):\n",
    "    lasso = Lasso(alpha=lmbda, tol=.1)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    r2 = lasso.score(X_train, y_train)\n",
    "    r2t = lasso.score(X_test, y_test)\n",
    "    print(f\"lambda={lmbda:4d}: Zeros {sum(np.abs(lasso.coef_) < 1e-5):3d}: train R^2 {r2:.2f} test R^2 {r2t:.2f}\")\n",
    "    plotbeta(lasso.coef_, f\"$\\\\lambda={lmbda}$\\n\", ax=axes[i], fontsize=9, xlabel=False, ylabel=i==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why does the training R^2 score go down as we increase lambda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "We are trading a bit of bias for improve generality. As we increase $\\lambda$, we are restricting how close to the true minimum loss function location we can get.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why does the L1 testing R^2 score go up we increase lambda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "As we constrain the coefficients, we reduce the overfitting to the training data and hence generality improves.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Describe what is happening to the L1 coefficients as we increase lambda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The average magnitude of the coefficients is not changing much, but they are becoming a tighter group; the standard deviation is shrinking significantly. Many more coefficients are going to zero. Note that about 80% of the coefficients  have gone to zero when we get the best test score.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,7,figsize=(13.5,1.8), sharey=True)\n",
    "for i,lmbda in enumerate([1,10,100,200,500,1000,2000]):\n",
    "    ridge = Ridge(alpha=lmbda, tol=.1)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    r2 = ridge.score(X_train, y_train)\n",
    "    r2t = ridge.score(X_test, y_test)\n",
    "    print(f\"lambda={lmbda:4d}: Zeros {sum(np.abs(ridge.coef_) < 1e-5)}: train R^2 {r2:.2f} test R^2 {r2t:.2f}\")\n",
    "    plotbeta(ridge.coef_, f\"$\\\\lambda={lmbda}$\\n\", ax=axes[i], fontsize=9, xlabel=False, ylabel=i==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Why does the training R^2 score go down as we increase lambda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "We are introducing bias as we did for L1, in exchange for increased generality.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Describe what is happening to the L2 coefficients as we increase lambda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    " The average magnitude of the coefficients goes down and they become a tighter group, but not as quickly as L1. We don't get any more coefficients going to zero.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Characterize how the magnitudes of L1 and L2 coefficients differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "   The L2 coefficients are in general much tighter group than the L1, even as we increase $\\lambda$. The L1 regularization drops many of the coefficients to zero for the same value of $\\lambda$.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
